# -*- coding: utf-8 -*-
"""NLP Model

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pkDD9vMUCLqJyMdSnikpNysL-gBl2hJz

Mikail Crito Husada
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
df = pd.read_csv('imdb_indonesian_movies_2.csv')

df.head()

df.shape

df.isna().sum()

df.genre.value_counts()
sns.set_style("darkgrid")

print(df.info())

df['text'] = df['ringkasan_sinopsis'] + " " + df['genre']
df

"""Cleaning Data"""

from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize,sent_tokenize
from bs4 import BeautifulSoup
import re,string,unicodedata
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.model_selection import train_test_split
from string import punctuation
from nltk import pos_tag
from nltk.corpus import wordnet
import keras
from keras.preprocessing import text, sequence
import nltk
nltk.download('stopwords')

stwd = set(stopwords.words('english'))
punctuation = list(string.punctuation)
stwd.update(punctuation)

def strip_html(text):
  soup = BeautifulSoup(text, "html.parser")
  return soup.get_text()

def remove_between_square_brackets(text):
  return re.sub('\[[^]]*\]', '', text)
def remove_url(text):
  return re.sub(r'http\S+', '', text)

def remove_stopwords(text):
  final_text=[]
  for i in text.split():
    if i.strip().lower() not in stwd:
      final_text.append(i.strip())
  return " ".join(final_text)

def denoise_text(text):
  text = strip_html(text)
  text = remove_between_square_brackets(text)
  text = remove_url(text)
  text = remove_stopwords(text)
  return text

df['text'] = df['text'].apply(denoise_text)

"""Jumlah Kata"""

from typing import Counter
def get_corpus(text):
  words = []
  for i in text:
    for j in i.split():
      words.append(j.strip())
  return words
corpus = get_corpus(df.text)
corpus[:10]

# kata yang sering ditemukan
from collections import Counter
counter = Counter(corpus)
most_common = counter.most_common(10)
most_common = dict(most_common)
most_common

from sklearn.feature_extraction.text import CountVectorizer
def get_top_text_ngrams(corpus, n, g):
  cv = CountVectorizer(ngram_range=(g, g)).fit(corpus)
  bag_words = cv.transform(corpus)
  sum_words = bag_words.sum(axis=0)
  words_freq = [(word, sum_words[0, idx]) for word, idx in cv.vocabulary_.items()]
  words_freq = sorted(words_freq, key = lambda x: x[1], reverse=True)
  return words_freq[:n]

plt.figure(figsize = (16,9))
most_common = get_top_text_ngrams(df.text,10,1)
most_common = dict(most_common)
sns.barplot(x=list(most_common.values()), y=list(most_common.keys()))

"""Encoding & Splitting Data"""

#One-Hot-Encoding
genre= pd.get_dummies(df.genre)
new_cat = pd.concat([df, genre], axis=1)
new_cat = new_cat.drop(columns='genre')
new_cat.head(10)

"""Change Dataframe to Numpy Array"""

sinopsis = new_cat['text'].values
label = new_cat[['Drama', 'Horor', 'Komedi', 'Laga', 'Romantis']].values

#cek news
sinopsis

#cek label
label

"""Split Dataset"""

x_train, x_test, y_train, y_test = train_test_split(sinopsis, label, test_size = 0.2, shuffle=True)

"""Tokenizer dan Permodelan Sequential dengan Embedding dan LSTM"""

import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.layers import LSTM, Dense, Embedding, Dropout
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam

vocab_size = 10000
max_len = 200
trunc_type = "post"
oov_tok = "<OOV>"

tokenizer = Tokenizer(num_words = vocab_size, oov_token = oov_tok)
tokenizer.fit_on_texts(x_train)

word_index = tokenizer.word_index

sequence_train = tokenizer.texts_to_sequences(x_train)
sequence_test = tokenizer.texts_to_sequences(x_test)
pad_train = pad_sequences(sequence_train, maxlen=max_len, truncating=trunc_type)
pad_test = pad_sequences(sequence_test, maxlen=max_len, truncating=trunc_type)

print(pad_test.shape)

pad_train

pad_test

"""MODEL"""

model = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=64, input_length = max_len),
    tf.keras.layers.LSTM(128),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(5, activation='softmax')
])
model.compile(optimizer='adam', metrics=['accuracy'], loss='categorical_crossentropy',)
model.summary()

"""Callback"""

class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs=()):
    if(logs.get('accuracy')>0.90 and logs.get('val_accuracy')>0.90):
      self.model.stop_training=True
      print("\n akurasi dari training set dan validation set telah terpenuhi > 90%")
callbacks = myCallback()

num_epochs = 50
history = model.fit(pad_train, y_train, epochs=num_epochs,
                    validation_data=(pad_test, y_test), verbose=2, callbacks=[callbacks])

"""Grafik"""

#Accuracy
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model Accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='lower left')
plt.show()

#plot of loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model Loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='lower right')
plt.show()